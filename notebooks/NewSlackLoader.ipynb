{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "import datetime\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from slack_data_loader2 import SlackLoader2, normalize_links\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.stem.snowball import EnglishStemmer, RussianStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import eli5\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge, RidgeClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.metrics import mean_squared_log_error, roc_auc_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifierCV\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, cross_val_score\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data\n",
    "* \"channel\"\n",
    "* \"start_message_text\"\n",
    "* \"thread_text\" (__eou__ __eot__)\n",
    "* \"message_type\" = [\"top\", \"first_thread\", \"thread\"]\n",
    "* \"attachment\" = 1|0\n",
    "* \"message_number\" = (1 - top,)\n",
    "* \"message_text\" \n",
    "### _target_\n",
    "emoji, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "751861\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = Path(\"../input/export_Feb_8_2018\")\n",
    "\n",
    "channels = ['_jobs', 'career']\n",
    "\n",
    "loader = SlackLoader2(BASE_DIR, exclude_channels=[])#, only_channels=channels)\n",
    "print(len(loader.messages))\n",
    "\n",
    "#df = pd.DataFrame(loader.threads).set_index([\"channel_name\", \"start_ts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 12897/751861 [00:01<01:35, 7749.51it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty text {'text_': '', 'text': None, 'attachments': [{'pretext': 'There are no events today.', 'markdwn_in': ['pretext']}], 'channel': 'C04422A5C', 'dt': datetime.datetime(2015, 8, 21, 18, 59, 59, 431), 'type': 'message', 'channel_name': '_meetings', 'subtype': 'bot_message', 'bot_id': 'B09DR8T8S', 'ts': 1440172799.000431}\n",
      "Empty text {'text_': '', 'text': None, 'attachments': [{'pretext': 'There are no events today.', 'markdwn_in': ['pretext']}], 'channel': 'C04422A5C', 'dt': datetime.datetime(2015, 8, 31, 6, 59, 59, 191), 'type': 'message', 'channel_name': '_meetings', 'subtype': 'bot_message', 'bot_id': 'B09DR8T8S', 'ts': 1440993599.000191}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 146332/751861 [02:21<19:58, 505.24it/s]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "loader.process_threads()\n",
    "print(len(loader.threads))\n",
    "df = pd.DataFrame(loader.threads).set_index([\"channel_name\", \"start_ts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_cache = \\\"all_threads.pkl.compress\\\"\\n\",\n",
    "    \"#df.to_pickle(file_cache, compression=\\\"gzip\\\")\\n\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.DataFrame(loader.messages)\n",
    "\n",
    "def add_punkt_nums(df, text_col):\n",
    "    items = [\"words\", \"commas\", \"bangs\", \"quotas\"]\n",
    "    patterns = {\"words\":'\\S+', \n",
    "                \"commas\":'\\,',\n",
    "                \"dots\":'\\.',\n",
    "                \"bangs\":'\\!',\n",
    "                \"questions\":'\\!',\n",
    "                \"quotas\":'\\\"', \n",
    "                \"apost\":'\\`'\n",
    "               }\n",
    "    for name, pat in patterns.items():\n",
    "        df['num_' + name] = df[text_col].str.count(pat)\n",
    "    df['avg_word'] = df[text_col].str.len() / (1 + df[\"num_words\"])\n",
    "    return df\n",
    "\n",
    "#df = add_punkt_nums(df, \"text_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['attachments', 'bot_id', 'bot_link', 'channel', 'channel_id',\n",
       "       'channel_name', 'client_msg_id', 'comment', 'display_as_bot', 'dt',\n",
       "       'edited', 'file', 'hidden', 'icons', 'inviter', 'is_intro',\n",
       "       'is_multiteam', 'is_org_shared', 'is_shared', 'is_thread_broadcast',\n",
       "       'item', 'item_type', 'members', 'mrkdwn', 'name', 'new_broadcast',\n",
       "       'no_notifications', 'old_name', 'parent_user_id', 'permalink',\n",
       "       'plain_text', 'purpose', 'reactions', 'reactions_', 'replies',\n",
       "       'reply_count', 'room', 'root', 'slog_is_mpdm', 'slog_is_self_dm',\n",
       "       'slog_is_shared', 'slog_is_slackbot_dm', 'slog_local_team_id',\n",
       "       'subtype', 'text', 'text_', 'thread_ts', 'timestamp', 'topic', 'ts',\n",
       "       'type', 'unfurl_links', 'unfurl_media', 'unread_count', 'upload',\n",
       "       'upload_reply_to', 'user', 'username', 'num_questions', 'num_bangs',\n",
       "       'num_words', 'num_quotas', 'num_dots', 'num_apost', 'num_commas',\n",
       "       'avg_word'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/algis/.local/lib/python3.5/site-packages/ipykernel/__main__.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13408 rows droped.\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "\n",
    "def get_ktop_reactions_threads(df, k=20):\n",
    "    df[\"reactions_\"] = df[\"reactions_\"].apply(lambda x : dict(x) )\n",
    "    df2 = df[\"reactions_\"].apply(pd.Series)\n",
    "    df3 = pd.concat([df, df2], axis=1).drop('reactions_', axis=1)\n",
    "    react_stat = df3.drop([\"msg_counter\"], axis=1).describe().transpose().sort_values([\"count\"], ascending=False)\n",
    "    #df_[df_[\"count\"] >= 100] #get by threshold\n",
    "    cols_to_del = react_stat[k:].index.values\n",
    "    ktop_cols = react_stat[:k].index.values\n",
    "    was_rows = df3.shape[0]\n",
    "    df3 = df3.drop(cols_to_del, axis=1).dropna(axis=0, subset=ktop_cols, how='all')\n",
    "    print(\"{} rows droped.\".format(was_rows-df3.shape[0]))\n",
    "    return df3, ktop_cols\n",
    "\n",
    "def get_ktop_reactions_messages(df, k=20):\n",
    "    COL_PREFIX = \"reaction_\"\n",
    "    df[\"reactions_\"] = df[\"reactions_\"].apply(lambda x : dict(x) )\n",
    "    df2 = df[\"reactions_\"].apply(pd.Series).add_prefix(COL_PREFIX)\n",
    "    df3 = pd.concat([df, df2], axis=1).drop('reactions_', axis=1)\n",
    "    filter_col = [col for col in df3 if col.startswith(COL_PREFIX)]\n",
    "    react_stat = df3[filter_col].describe().transpose().sort_values([\"count\"], ascending=False)\n",
    "    #df_[df_[\"count\"] >= 100] #get by threshold\n",
    "    cols_to_del = react_stat[k:].index.values\n",
    "    ktop_cols = react_stat[:k].index.values\n",
    "    was_rows = df3.shape[0]\n",
    "    df3 = df3.drop(cols_to_del, axis=1).dropna(axis=0, subset=ktop_cols, how='all')\n",
    "    print(\"{} rows droped.\".format(was_rows-df3.shape[0]))\n",
    "    return df3, ktop_cols, react_stat\n",
    "\n",
    "\n",
    "reacts, ktop_cols, react_stat = get_ktop_reactions_messages(df.dropna(subset=[\"reactions_\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26808, 85)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reacts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['reaction_+1', 'reaction_heavy_plus_sign', 'reaction_joy',\n",
       "       'reaction_fire', 'reaction_notbad', 'reaction_trollface',\n",
       "       'reaction_100', 'reaction_povar', 'reaction_muscle',\n",
       "       'reaction_true-story', 'reaction_yeah-sure', 'reaction_ban',\n",
       "       'reaction_facepalm', 'reaction_slava', 'reaction_putin',\n",
       "       'reaction_be-a-man', 'reaction_but_why', 'reaction_kekeke',\n",
       "       'reaction_wat', 'reaction_ternaus'], dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ktop_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = reacts['text'].fillna(\"_na_\").values\n",
    "y = reacts[ktop_cols].fillna(0).values\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ufal.udpipe import Model, Pipeline, ProcessingError # pylint: disable=no-name-in-module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pipeline(modelfile='/media/data/word2vec/ufal/udpipe-ud-2.0-170801/russian-syntagrus-ud-2.0-170801.udpipe'):\n",
    "    model = Model.load(modelfile)\n",
    "    pipeline = Pipeline(model, 'tokenize', Pipeline.DEFAULT, Pipeline.DEFAULT, 'conllu')\n",
    "    return pipeline\n",
    "\n",
    "pipeline = get_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['&gt; понятно, пойду значит дальше сам писать\\nТак ты внимания просто хотел?',\n",
       "       '<@U04ELQZAU> а можно ссылочку на канал?',\n",
       "       'мы используем zeppelin + scala при разработке джоб, дебаггинге.\\nаналитики и data scientists тоже используют, но они обычно выгружают оттуда данные в jupyter',\n",
       "       'я однажды в британии сказал барышне из гугла, что она бы неплохо на кухне смотрелась',\n",
       "       '<https://github.com/deepmind/lab>',\n",
       "       'Сходила на собеседование в LinkedIn, получила в качестве сувенира распечатанный граф своих контактов. Синий клубок слева - ODS-circlejerk.\\n<https://photos.app.goo.gl/JSjyyfi8FBci79HI3>',\n",
       "       '<https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Generative-Adversarial-Networks>',\n",
       "       'thefuck для Bash: <https://github.com/nvbn/thefuck>',\n",
       "       'Если будет еще с пяток желающих - я думаю, можно смело создавать.',\n",
       "       '<https://media.giphy.com/media/GMIbzgzyS4pws/giphy.gif>',\n",
       "       'Баян, но всё же. Есть вот такая статья «The Marginal Value of Adaptive Gradient Methods in Machine Learning», <https://arxiv.org/abs/1705.08292> . В ней сравнивают работу разных оптимизаторов: SGD, SGD+momentum, RMSProp, AdaGrad, Adam. Общий посыл статьи такой – адаптивные методы (RMSProp, Adam, AdaGrad) работают у авторов хуже, чем обычный SGD и он же с моментумом. Они проверяют на разных доступных в интернете сетках и задачах диплернинга, таких как распознавание картинок, рекуррентных языковых моделях и т.п. И везде у них SGD типа лучше всех ведет себя и на тесте, и на трейне – и дает лучшую точность, и даже быстрее сходится. А адаптивные методы могут быть сравнимы с SGD на трейне, но точно проигрывают на тесте. Причем адам ведет себя хуже всех. Объясняют они это тем, что адаптивные методы сильнее оверфитятся и хуже обобщают. Даже что-то там доказывают про это в начале статьи на задаче линейной регрессии. Однако при сравнении они делают для каждого метода очень точный подбор лернингрейта и кривой для снижения лернингрейта. И говорят, что именно правильный подбор этого делает SGD таким успешным. А адаптивные методы, как я понял, работают и с более грубым подходом к заданию learning rate и его decay, сходятся быстро и почти хорошо, но все равно всегда хуже, чем правильно настроенный SGD. Статья, конечно, холиварная и доверять ей сходу стрёмно, но в каждой сказке, как говорится, есть намёк… Тред, где затрагивался этот вопрос и эта статья вот <https://opendatascience.slack.com/archives/C047H3N8L/p1506093896000634>',\n",
       "       'а я думал ценность деньгам дает воображение, а в определенные периоды (1998) наступает просветление))',\n",
       "       'Хотя хз, что это вообще',\n",
       "       'Мне кажется они охуели в край, я бы на месте пацанов, кого использовали без спросу в этом очевидно мошенническом документе принял бы меры <https://static.mirocana.com/documents/en/WhitePaper.pdf>',\n",
       "       'Нет, это реально не я писал',\n",
       "       '<https://i.redd.it/1d1vcexiezsz.jpg>',\n",
       "       'Даже у меркель есть chemistry. А у тебя нет. ',\n",
       "       'ваши ставки, господа',\n",
       "       'Так это правда, что первым 500 окончившим книгу deep learning mail подарит? :grinning:',\n",
       "       'и 6-страничным договором',\n",
       "       'Я никогда не участвовал. Откуда у меня cool story, про это.\\n\\nВ теории так как времени мало, кстати, сколько на разговор? \\n\\nВариации на тему когда можно работать тоньше, чтобы у нее сложилось впечатление, что когда она рядом с тобой - она чувствует себя special нет. Работать на ее любопытстве, так чтобы у девушки в голове на подсознательном уровне выплывало, что ты не такой как все в хорошем смысле этого слова - нет. Работать на ее  insecurities, так чтобы она чувствовала себя в твоем присутствии комфортно, безопасно, как за каменной стеной - тоже требует времени. Рассказывать байки, так чтобы она искренне смеялась (очень сильный показатель) - нужно ну минут 15, как минимум. И т.д.\\n\\nКороче есть подозрение, что надо тупо бить по chemistry и body language, ибо другого выбора нет. Но это все теории. Тут надо пробовать.',\n",
       "       'Я тоже не понимаю, что плохого в том, что москвичу платят больше, чем новгородцу. Тут столько определяющих факторов, что нельзя сводить к: \"вы там в москве совсем охуели\" и \"это блять дискриминация по географическому признаку\"',\n",
       "       \"<@U4X5SNMK2> - все.\\n\\nСобственно переобпределение сети у меня выглядит вот так:\\n```\\ndef get_model(num_classes, model_type='resnet50'):\\n    if model_type == 'resnet50':\\n        model = resnet50(pretrained=True).cuda()\\n        model.fc = nn.Linear(model.fc.in_features, num_classes).cuda()\\n    elif model_type == 'resnet101':\\n        model = resnet101(pretrained=True).cuda()\\n        model.fc = nn.Linear(model.fc.in_features, num_classes).cuda()\\n    elif model_type == 'resnet152':\\n        model = resnet152(pretrained=True).cuda()\\n        model.fc = nn.Linear(model.fc.in_features, num_classes).cuda()\\n    elif model_type == 'densenet121':\\n        model = densenet121(pretrained=True).cuda()\\n        model.classifier = nn.Linear(model.classifier.in_features, num_classes).cuda()\\n    elif model_type == 'densenet161':\\n        model = densenet161(pretrained=True).cuda()\\n        model.classifier = nn.Linear(model.classifier.in_features, num_classes).cuda()\\n    elif model_type == 'densenet201':\\n        model = densenet201(pretrained=True).cuda()\\n        model.classifier = nn.Linear(model.classifier.in_features, num_classes).cuda()\\n    return model\\n```\",\n",
       "       'Это, наверное, сюда ) <https://habrahabr.ru/post/336390/>',\n",
       "       '<https://youtu.be/0R58_tx7azY>',\n",
       "       '<@U1CF22N7J> =&gt; Competitions Expert :troll:',\n",
       "       'Да, работа через ИП, + надо проходить валютный контроль при каждом платеже, т.к. переводы из Ирландии. Ничего особо сложного тут нет, у нас есть стандартный договор для России, который на многих банках опробован (в фирме человек 15 из РФ сейчас работает). С налогами и бухгалтерией чтоб не запутаться - когда-что платить, какую отчетность сдавать, как все это без походов в налоговую делать, можно Эльбу использовать. Это все доп. расходы, но небольшие (от 1% до 6% налогов, в зависимости от региона, + 1-2тр в месяц за банк и валютный контроль, +4тр в год за бухгалтерию). Плюс время, у меня на это  где-то пол-часа - час в месяц уходит.',\n",
       "       'Как вам такое? <https://www.tensorflow.org/tfrc/>',\n",
       "       '<@U09JEC7V0>:  Мы не юзали Azure, но встречались с сейлзами из Microsoft месяца 3 назад. Начальству было интересно посмотреть на Hadoop в графическом интерфейсе. В итоге… плюсов не нашли))\\n•\\tВ графическом интерфейсе можно только развернуть кластер на виртуалках, жестко заданной конфигурации.\\n•\\tУ них странный подход к кластерам. Есть кластер Hadoop, куда входят HDFS, Hive и еще несколько базовых утилит. Есть кластер Spark, есть кластер HBase, есть кластер Storm. Каждый кластер разворачивается отдельно и отдельно оплачивается. На мой вопрос и как же этим пользоваться если мне нужен и Hadoop и Spark они затруднились ответить (позже сообщили, что с помощью специальных скриптов можно поднять другие сервисы)\\n•\\tИспользуется HDP 2.2, когда планируется переход на 2.3 неизвестно (хотя 2.3 доступен с лета 2015).\\nSLA обещают 99,999% - данные могут быть недоступны максимум 20 минут в месяц.\\nПо поводу оплаты, примерные расчеты:\\nХранение 500 Гб в облаке - 1250 руб/мес\\nКластер Hadoop (7 нод, 16 CPU, RAM 112 Gb) - 330 000 руб/мес\\nБазовая поддержка - 1500 руб/мес\\nПользование кластером тарифицируется поминутно, т.е. если ночью не пользуешься, то его можно выключить и не платить. 330 тыс это если круглосуточно пользоваться.',\n",
       "       'онлайн-трансляция <https://new.vk.com/deeplearning?z=video-11283947_456239033%2Fcf6ccead745974d95f%2Fpl_post_-11283947_75609>'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "in method 'Pipeline_process', argument 2 of type 'std::string const &'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-70126d4e494c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocessed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'#'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtagged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: in method 'Pipeline_process', argument 2 of type 'std::string const &'"
     ]
    }
   ],
   "source": [
    "processed = pipeline.process()\n",
    "output = [l for l in processed.split('\\n') if not l.startswith('#')]\n",
    "tagged = ['_'.join(w.split('\\t')[2:4]) for w in output if w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.6 s, sys: 0 ns, total: 31.6 s\n",
      "Wall time: 31.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ru_stemmer = RussianStemmer()\n",
    "en_stemmer = EnglishStemmer()\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "def is_russian_word(w):\n",
    "    #if not isinstance(w, unicode):\n",
    "    #    w = w.decode('utf8')\n",
    "    for c in w:\n",
    "        if ord(c) >= ord(u'а') and ord(c) <= ord(u'я'):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    for w in analyzer(doc):\n",
    "        yield ru_stemmer.stem(w) if is_russian_word(w) else en_stemmer.stem(w)\n",
    "        \n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=5, analyzer=stemmed_words).fit(X_train)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.2 s, sys: 172 ms, total: 25.4 s\n",
      "Wall time: 25.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# we need a custom pre-processor to extract correct field,\n",
    "# but want to also use default scikit-learn preprocessing (e.g. lowercasing)\n",
    "default_preprocessor = CountVectorizer().build_preprocessor()\n",
    "\n",
    "import re, string\n",
    "re_tok = re.compile('([{}“”¨«»®´·º½¾¿¡§£₤‘’])'.format(string.punctuation))\n",
    "def tokenize(s): return re_tok.sub(r' \\1 ', s).split()\n",
    "  \n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n",
    "               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
    "               smooth_idf=1, sublinear_tf=1).fit(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 44s, sys: 556 ms, total: 1min 44s\n",
      "Wall time: 1min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='char',\n",
    "    ngram_range=(1, 5),\n",
    "    max_features=20000).fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ящик 9990\n",
      "ячейк 9989\n",
      "ясн 9988\n",
      "яростн 9987\n",
      "ярк 9986\n",
      "японск 9985\n",
      "яндекс 9984\n",
      "январ 9983\n",
      "ян 9982\n",
      "якоб 9981\n",
      "яйц 9980\n",
      "языков 9979\n",
      "язык 9978\n",
      "ядр 9977\n",
      "ядерн 9976\n",
      "ядер 9975\n",
      "явн 9974\n",
      "явля 9973\n",
      "явлен 9972\n",
      "явк 9971\n"
     ]
    }
   ],
   "source": [
    "for _word, _count in sorted(vectorizer.vocabulary_.items(),key=itemgetter(1),reverse=True)[:20]:\n",
    "    print(_word,end=' ')\n",
    "    print(_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def svc_param_selection(X, y, nfolds):\n",
    "    Cs = [0.001, 0.01, 0.1, 1, 10]\n",
    "    param_grid = {'C': Cs, 'dual': [False], 'penalty':['l1','l2'], 'class_weight': [None, 'balanced']}\n",
    "    #param_grid = {'C': Cs}\n",
    "    model = LinearSVC(random_state=0)\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=nfolds, n_jobs=1, scoring='roc_auc', verbose=1)\n",
    "    grid_search.fit(X, y)\n",
    "    #grid_search.best_params_\n",
    "    return grid_search\n",
    "  \n",
    "\n",
    "gs_result = svc_param_selection(X_train_vec, y_train, 3)\n",
    "\n",
    "print(gs_result.best_params_, gs_result.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34.3 s, sys: 0 ns, total: 34.3 s\n",
      "Wall time: 34.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "X_train_vec = vectorizer.transform(X_train)\n",
    "\n",
    "\n",
    "X_val_vec = vectorizer.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start...\n",
      "valid rmsle: 5.587547840901913\n",
      "valid rmsle: 5.229081737420327\n",
      "valid rmsle: 4.598652551267773\n",
      "valid rmsle: 4.895742447573883\n",
      "valid rmsle: 5.2448290711965\n",
      "valid rmsle: 5.069583814199927\n",
      "valid rmsle: 4.963362026046199\n",
      "valid rmsle: 5.498358699219086\n",
      "valid rmsle: 4.785797118279256\n",
      "valid rmsle: 5.521744595744533\n",
      "Make prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/algis/.local/lib/python3.5/site-packages/ipykernel/__main__.py:68: RuntimeWarning: invalid value encountered in power\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-7c82e66f9410>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0mtest_predicts\u001b[0m \u001b[0;34m**=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_rmsle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_predicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-40-7c82e66f9410>\u001b[0m in \u001b[0;36mget_rmsle\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_rmsle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_log_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_roc_auc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/metrics/regression.py\u001b[0m in \u001b[0;36mmean_squared_log_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \"\"\"\n\u001b[1;32m    307\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0;32m--> 308\u001b[0;31m         y_true, y_pred, multioutput)\n\u001b[0m\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/metrics/regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \"\"\"\n\u001b[1;32m     75\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    451\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    452\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     42\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     43\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 44\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "def pr(y_i, y, x):\n",
    "    p = x[y==y_i].sum(0)\n",
    "    return (p+1) / ((y==y_i).sum()+1)\n",
    "  \n",
    "def get_mdl(X, y):\n",
    "    #y = y.values\n",
    "    r = np.log(pr(1,y, X) / pr(0,y, X))\n",
    "    m = LogisticRegression(C=4, dual=True)\n",
    "    x_nb = X.multiply(r)\n",
    "    return m.fit(x_nb, y), r  \n",
    "\n",
    "def get_rmsle(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_log_error(np.expm1(y_true), np.expm1(y_pred)))\n",
    "\n",
    "def get_roc_auc(y_true, y_pred):\n",
    "    return roc_auc_score(y_true, y_pred)\n",
    "  \n",
    "def ridge():\n",
    "    \n",
    "    return RidgeClassifier(\n",
    "        solver='auto',\n",
    "        fit_intercept=True,\n",
    "        alpha=0.5,\n",
    "        max_iter=100,\n",
    "        normalize=False,\n",
    "        tol=0.05)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def linear_svm():\n",
    "    C = 0.01\n",
    "    return LinearSVC(penalty='l1', dual=False, C=C, random_state=0)\n",
    "\n",
    "def get_y(y_train, index = 1):\n",
    "    y_ = y_train[:,0]#.reshape(-1,1)\n",
    "    return y_ #np.where(y_ > 0, 1, 0)\n",
    "\n",
    "#X_train_vec = X_train_vec#.toarray()\n",
    "y_ = get_y(y_train)\n",
    "\n",
    "cv = KFold(n_splits=10, shuffle=False, random_state=42)\n",
    "models = []\n",
    "print(\"start...\")\n",
    "for train_ids, valid_ids in cv.split(X_train_vec):\n",
    "    #model = linear_svm()\n",
    "    #model = linear_model.BayesianRidge()\n",
    "    model = linear_model.Ridge(alpha = .5)\n",
    "    #model = linear_model.Lasso(alpha = 0.1)\n",
    "    #model = linear_model.SGDRegressor()\n",
    "    #model = linear_model.ElasticNet()\n",
    "    #model = GradientBoostingRegressor()\n",
    "    #RandomForestClassifier, RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "    model.fit(X_train_vec[train_ids], y_[train_ids])\n",
    "    y_pred_valid = model.predict(X_train_vec[valid_ids])\n",
    "    rmsle = get_rmsle(y_pred_valid, y_[valid_ids])\n",
    "    print('valid rmsle: {}'.format(rmsle))\n",
    "    models.append(model)\n",
    "    \n",
    "print(\"Make prediction\")\n",
    "\n",
    "\n",
    "\n",
    "test_predicts = np.ones(y_val.shape[0])\n",
    "for m in models:\n",
    "    test_predicts *= m.predict(X_val_vec)#.toarray())\n",
    "\n",
    "test_predicts **= (1. / len(models))\n",
    "\n",
    "print(get_rmsle(test_predicts, get_y(y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid rmsle: 5.0328482482568475\n"
     ]
    }
   ],
   "source": [
    "def get_rmsle(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_log_error(np.expm1(y_true), np.expm1(y_pred)))\n",
    "\n",
    "def get_ridge():\n",
    "    return Ridge(\n",
    "        solver='auto',\n",
    "        fit_intercept=True,\n",
    "        alpha=0.5,\n",
    "        max_iter=100,\n",
    "        normalize=False,\n",
    "        tol=0.05)\n",
    "\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "for train_ids, valid_ids in cv.split(X_train_vec):\n",
    "    model = get_ridge()#RandomForestRegressor()\n",
    "    model.fit(X_train_vec[train_ids], y_[train_ids])\n",
    "    y_pred_valid = model.predict(X_train_vec[valid_ids])\n",
    "    rmsle = get_rmsle(y_pred_valid, y_[valid_ids])\n",
    "    print('valid rmsle: {}'.format(rmsle))\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        \n",
       "\n",
       "    \n",
       "\n",
       "        \n",
       "            \n",
       "                \n",
       "                \n",
       "    \n",
       "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
       "            <b>\n",
       "    \n",
       "        y\n",
       "    \n",
       "</b>\n",
       "\n",
       "top features\n",
       "        </p>\n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; margin-bottom: 2em;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature weights. Note that weights do not account for feature value scales, so if feature values have different scales, features with highest weights might not be the most important.\">\n",
       "                    Weight<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +35.932\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        everyon\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 82.14%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +30.570\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        осен\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 83.48%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +27.354\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        офер\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 84.11%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +25.861\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        pacman\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 85.11%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +23.573\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        стыд\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 85.50%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +22.699\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        hbase\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 86.83%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +19.780\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        speaker\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 86.86%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +19.725\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        druid\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 87.07%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +19.268\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        монстр\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 87.32%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +18.731\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        подписа\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 87.35%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +18.673\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        аспект\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 87.55%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +18.249\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        xor\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 87.63%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +18.084\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        стартанул\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 87.87%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +17.590\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        попиар\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 87.87%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +17.585\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        c074f6e1k\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 88.05%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +17.217\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        банхаммер\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 88.23%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +16.857\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        руковож\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 88.30%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +16.699\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        deephack\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 88.33%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +16.652\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        ан\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 88.40%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +16.505\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        барселон\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 88.40%); border: none;\">\n",
       "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none;\">\n",
       "                    <i>&hellip; 5488 more positive &hellip;</i>\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 80.00%); border: none;\">\n",
       "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none;\">\n",
       "                    <i>&hellip; 4484 more negative &hellip;</i>\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "            \n",
       "        \n",
       "\n",
       "        \n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5.show_weights(model, vec=vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-706d550917eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m#y = reacts[ktop_cols].fillna(0).values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.sparse import hstack\n",
    "from scipy.special import logit, expit\n",
    "from tqdm import tqdm\n",
    "\n",
    "class_names = ktop_cols\n",
    "\n",
    "train_text = X_train\n",
    "test_text = X_val\n",
    "all_text = np.concatenate([train_text, test_text])\n",
    "\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=15000)\n",
    "word_vectorizer.fit(all_text)\n",
    "train_word_features = word_vectorizer.transform(train_text)\n",
    "test_word_features = word_vectorizer.transform(test_text)\n",
    "\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='char',\n",
    "    ngram_range=(1, 5),\n",
    "    max_features=20000)\n",
    "char_vectorizer.fit(all_text)\n",
    "train_char_features = char_vectorizer.transform(train_text)\n",
    "test_char_features = char_vectorizer.transform(test_text)\n",
    "\n",
    "train_features = hstack([train_char_features, train_word_features])\n",
    "test_features = hstack([test_char_features, test_word_features])\n",
    "\n",
    "\n",
    "#submission = pd.DataFrame.from_dict(predictions)\n",
    "#submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "#predictions = {'id': test['id']}\n",
    "\n",
    "#y = reacts[ktop_cols].fillna(0).values\n",
    "\n",
    "\n",
    "def get_ridge():\n",
    "    return Ridge(\n",
    "        solver='auto',\n",
    "        fit_intercept=True,\n",
    "        alpha=0.5,\n",
    "        max_iter=100,\n",
    "        normalize=False,\n",
    "        tol=0.05)\n",
    "\n",
    "for index, class_name in tqdm(enumerate(class_names)):\n",
    "    #train_target = np.where(y_train[:,index] > 0, 1,0)\n",
    "    train_target = y_train[:,index]\n",
    "    #classifier = get_ridge()#LogisticRegression(solver='sag')\n",
    "    classifier = linear_model.Ridge(alpha = .5)\n",
    "    cv_loss = np.mean(cross_val_score(classifier, train_features, train_target, cv=5, scoring='neg_mean_squared_error'))\n",
    "    losses.append(cv_loss)\n",
    "    print('CV score for class {} is {}'.format(class_name, cv_loss))\n",
    "\n",
    "    #classifier.fit(train_features, train_target)\n",
    "    #predictions[class_name] = classifier.predict_proba(test_features)[:, 1]\n",
    "\n",
    "print('Total CV score is {}'.format(np.mean(losses)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
